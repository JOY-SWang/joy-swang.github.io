---
title: "RLPF: Towards Curriculum Reinforcement Learning with Psychologist-like Feedback."
collection: publications
category: manuscripts
permalink: /publication/PsyR1
excerpt: 'A reinforcement learning(RL) framework simulating clinical psychologist, incorporating an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically, going to be submitted to IEEE Transactions on Affective Computing (TAFFC), with *Dr. Ruiyuan Guan*.'
date: 2025-05-01
citation: 'Authors: Wang J., Xu D, Chao Y., ...\& Huang Y. A reinforcement learning(RL) framework simulating clinical psychologist, incorporating an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically, going to be submitted to IEEE Transactions on Affective Computing (TAFFC), with *Dr. Ruiyuan Guan*.'
---

Existing psychological large language models (LLMs) mainly depend on supervised fine-tuning and retrieval-augmented generation (RAG). They struggle to address dynamic reasoning demands in real-life tasks such as emotion recognition, diagnostic question-answering(QA), and counseling dialogues, as these tasks require simulating complex human cognitive processes such as multi-step causal reasoning and adaptive strategy optimization. Current models still have a gap in meeting these dynamic reasoning demands for real psychological counseling. To solve this problem, we propose an in-depth thinking LLM for psychology. We first constructed 8K verifiable medicopsychological problems and 8K counseling dialogues for training. Then, we presented a two-stage curriculum learning framework, first trained on exam-style problems to build solid reasoning and psychological knowledge, and then on counseling dialogues for dynamic strategy adaptation. Experiments show that our approach outperforms existing open-source general and psychological LLMs across benchmarks, demonstrating the efficacy of progressive training in handling complex tasks.

