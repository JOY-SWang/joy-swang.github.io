---
title: "Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering."
collection: publications
category: conferences
permalink: /publication/Mlevlm
excerpt: 'We propose MLeVLM, a Multi-level Visual Language Model for **Medical Visual Question Answering** focusing on recognition, details, diagnosis, knowledge, and reasoning. We construct a high-quality multi-level dataset (MLe-VQA) and a multi-level feature alignment module, and create a benchmark which MLeVLM outperforms.'
date: 2024-07-01
venue: 'ACL2024'
paperurl: 'https://aclanthology.org/2024.findings-acl.296.pdf'
citation: 'Dexuan Xu, Yanyuan Chen, Jieyi Wang, Yue Huang, Hanpin Wang, Zhi Jin, Hongxing Wang, Weihua Yue, Jing He, Hang Li, and Yu Huang. 2024. MLeVLM: Improve Multi-level Progressive Capabilities based on Multimodal Large Language Model for Medical Visual Question Answering. In Findings of the Association for Computational Linguistics: ACL 2024, pages 4977â€“4997, Bangkok, Thailand. Association for Computational Linguistics. We propose MLeVLM, a Multi-level Visual Language Model for **Medical Visual Question Answering** focusing on recognition, details, diagnosis, knowledge, and reasoning. We construct a high-quality multi-level dataset (MLe-VQA) and a multi-level feature alignment module, and create a benchmark which MLeVLM outperforms. '
---

Medical visual question answering (MVQA) requires in-depth understanding of medical images and questions to provide reliable answers. We summarize multi-level progressive capabilities that models need to focus on in MVQA: recognition, details, diagnosis, knowledge, and reasoning. Existing MVQA models tend to ignore the above capabilities due to unspecific data and plain architecture. To address these issues, this paper proposes Multi-level Visual Language Model (MLeVLM) for MVQA. On the data side, we construct a high-quality multi-level instruction dataset MLe-VQA via GPT-4, which covers multi-level questions and answers as well as reasoning processes from visual clues to semantic cognition. On the architecture side, we propose a multi-level feature alignment module, including attention-based token selector and context merger, which can efficiently align features at different levels from visual to semantic. To better evaluate the model's capabilities, we manually construct a multi-level MVQA evaluation benchmark named MLe-Bench. Extensive experiments demonstrate the effectiveness of our constructed multi-level instruction dataset and the multi-level feature alignment module. It also proves that MLeVLM outperforms existing medical multimodal large language models.