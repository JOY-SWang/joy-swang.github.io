---
title: "MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models"
collection: publications
category: manuscripts
permalink: /publication/MedMKEB
excerpt: 'We constructed a **medical multimodal knowledge-editing benchmark** with FT, KE, IKE, MEND, SERAC, and evaluated on 10+ knowledge-editing models, drawing on cognitive science and the four-phase clinical care.'
date: 2025-08-01
venue: 'AAAI2026'
paperurl: 'https://arxiv.org/pdf/2508.05083'
citation: 'Xu D., Wang J., Chai Z. ...\& Hu Y. MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models (with Dr. Huamin Zhang}), arXiv preprint, arXiv:2508.05083. We constructed a **medical multimodal knowledge-editing benchmark** with FT, KE, IKE, MEND, SERAC, and evaluated on 10+ knowledge-editing models, drawing on cognitive science and the four-phase clinical care.'
---

Recent advances in multimodal large language models (MLLMs) have significantly improved medical AI, enabling it to unify the understanding of visual and textual information. However, as medical knowledge continues to evolve, it is critical to enable these models to efficiently update outdated or incorrect information without retraining from scratch. Although textual knowledge editing has been widely studied, there is still a lack of systematic benchmarks for multimodal medical knowledge editing involving image and text modalities. To fill this gap, we present MedMKEB, the first comprehensive benchmark designed to evaluate the reliability, generality, locality, portability, and robustness of knowledge editing in medical multimodal large language models. MedMKEB is built on a high quality medical visual question-answering dataset and enriched with carefully constructed editing tasks including counterfactual correction, semantic generalization, knowledge transfer, and adversarial robustness. We incorporate human expert validation to ensure the accuracy and reliability of the benchmark. Extensive experiments on state-of-the-art general and medical MLLMs demonstrate the limitations of existing knowledge editing methods in the medical domain, highlighting the need to develop specialized editing strategies. MedMKEB will serve as a standard benchmark to promote the development of trustworthy and efficient medical knowledge editing algorithms.